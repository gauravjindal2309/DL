{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras as k\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/home/nancy/Downloads/questions_data_for_assignment.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns like id,qid1,qid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.drop(data.columns[0:3],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab=pd.read_csv('/home/nancy/Downloads/glove.6B/vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab=vocab.drop(vocab.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab=vocab.drop(vocab.columns[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.index=vocab.index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_id={}\n",
    "for i in range(len(vocab)):\n",
    "    word_id[vocab.loc[i+1,'word']]=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemming=PorterStemmer()\n",
    "\n",
    "            \n",
    "def doc_to_id(h):\n",
    "    l=[]\n",
    "    m=[]\n",
    "    a=nltk.tokenize.word_tokenize(h)\n",
    "    for w in a:\n",
    "        try:\n",
    "            n1=lemmatizer.lemmatize(w.lower())\n",
    "            #n=stemming.stem(n1)\n",
    "            l.append(word_id[n1])\n",
    "        except:\n",
    "                m.append(w.lower())\n",
    "    return l,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:15: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "word_id_question1=[]\n",
    "word_id_question2=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        word_id_question1.append(doc_to_id(data['question1'][i]))\n",
    "        word_id_question2.append(doc_to_id(data['question2'][i]))\n",
    "    except:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem in rows like blank row etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop(data.index[[117511,261473]],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparision between normal text V/s lemmatization V/s Stemming and lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_id_question1=[]\n",
    "word_id_question2=[]\n",
    "wrong_words=[]\n",
    "for i in range(len(data)):\n",
    "    l1,m1=doc_to_id(data['question1'][i])\n",
    "    l2,m2=doc_to_id(data['question2'][i])\n",
    "    \n",
    "    word_id_question1.append(l1)\n",
    "    word_id_question2.append(l2)\n",
    "    for j in m1:\n",
    "                    wrong_words.append(j)\n",
    "    for k in m2:\n",
    "                    wrong_words.append(k)\n",
    "                    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrong words in simple, lemmatize, lemmatize+stemming=88900,88608,68903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length1=[]\n",
    "sequence_length2=[]\n",
    "for i in range(len(word_id_question1)):\n",
    "    sequence_length1.append(len(word_id_question1[i]))\n",
    "    sequence_length2.append(len(word_id_question2[i]))\n",
    "\n",
    "#another way of computing sequence length\n",
    "#sequence_length1.append(len(nltk.word_tokenize(data[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(sequence_length1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most of sequence length is below 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_sentence_index=[]\n",
    "for i in range(len(sequence_length1)):\n",
    "    if(sequence_length1[i]>50):\n",
    "        remove_sentence_index.append(i)\n",
    "    elif(sequence_length2[i]>50):\n",
    "        remove_sentence_index.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1135"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_sentence_index)\n",
    "#len(sequence_length1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(data.index[remove_sentence_index],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100943\n",
      "298675\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sequence_length1)):\n",
    "    if(sequence_length1[i]==0):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263546\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sequence_length1)):\n",
    "    if(sequence_length2[i]==0):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(data.index[[263546,298675,100943]],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.drop(data.columns[0:2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('/home/nancy/Downloads/newdata.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:15: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "word_id_question1=[]\n",
    "word_id_question2=[]\n",
    "wrong_words=[]\n",
    "for i in range(len(data)):\n",
    "    l1,m1=doc_to_id(data['question1'][i])\n",
    "    l2,m2=doc_to_id(data['question2'][i])\n",
    "    \n",
    "    word_id_question1.append(l1)\n",
    "    word_id_question2.append(l2)\n",
    "    for j in m1:\n",
    "                    wrong_words.append(j)\n",
    "    for k in m2:\n",
    "                    wrong_words.append(k)\n",
    "sequence_length1=[]\n",
    "sequence_length2=[]\n",
    "for i in range(len(word_id_question1)):\n",
    "    sequence_length1.append(len(word_id_question1[i]))\n",
    "    sequence_length2.append(len(word_id_question2[i]))\n",
    "                    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1=np.zeros([len(sequence_length1),50],dtype=np.int)\n",
    "input2=np.zeros([len(sequence_length1),50],dtype=np.int)\n",
    "for i in range(len(sequence_length1)):\n",
    "    for j in range(sequence_length1[i]):\n",
    "        input1[i][j]=word_id_question1[i][j]\n",
    "    for k in range(sequence_length2[i]):\n",
    "        input2[i][k]=word_id_question2[i][k]\n",
    "#another of doing it using keras.preprocessing.sequence.pad_sequences(sequences, maxlen=50)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1=pd.DataFrame(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input2=pd.DataFrame(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input1.to_csv('/home/nancy/Downloads/inputque1')\n",
    "input2.to_csv('/home/nancy/Downloads/inputque2')\n",
    "\n",
    "#another way :no need to convert in dataframe use this ->  np.save(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding row in word_matrix for masking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding=pd.read_csv('/home/nancy/Downloads/glove.6B/vocab2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(np.zeros([1,100]),columns=word_embedding.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding.index=word_embedding.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding=df.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding.to_csv('/home/nancy/Downloads/glove.6B/vocab2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
